{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science ODL Project: Assessment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ID:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to the brief"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Aims, objectives and plan (4 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Aims and objectives\n",
    "A company has collected various attributes of the steel that they anneal, along with the attributes they have recorded the kind of annealing which was done previouly. \n",
    "They want to use these attributes and predict the type of Annealling that should be performed given a new instance of steel atrtibutes. \n",
    "The aim is develop a unbiased model which correctly predicts the annlealing class 98% of the times regardless of the class. In other words, if there 100 instances of each class, then the model \n",
    "should be able to correctly detect atleast 98 instances correctly from each class, which indicates that they want a model which has a very high True Positivity Rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  b) Plan\n",
    "Please demonstrate how you have conducted the project with a simple Gantt chart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding the case study (4 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Case study analysis\n",
    "State the key points that you found in the case and how you intend to deal with them appropriately to address the client's needs. (You can include more than four points.)\n",
    "\n",
    "\n",
    "1. Overview of the Data\n",
    "- The data is medium size with 798 training examples and 100 examples in test set. \n",
    "- There are 38 attributes, out which 6 are real-valued, 3 are oridnals and 29 categorical attributes. \n",
    "- The documentation of the data states that '-' represents the not_applicable values and '?' represent missing values. \n",
    "- Of the 29 categorical variables, 19 have binary values.\n",
    "- There are 6 documentated annealing (target) types. '1', '2', '3', '4', '5' and 'U' \n",
    "\n",
    "2. Class Imbalance\n",
    "    - Target distribution is heavily disblanced \n",
    "        - Class '3' dominates with 76% of the instances\n",
    "        - Class '2' is present in  11.4% of instances\n",
    "        - Class '5' is present in 7% of the instances\n",
    "        - Class 'U' is present in 4% of the instances\n",
    "        - Class '1' is present in 1% of the instances [only 8 examples]\n",
    "        - Class '4' is not present in the data.\n",
    "        - We will use synthetic minority over-sampling technique on the non-dominant classes.\n",
    "            - N. V. Chawla, K. W. Bowyer, L. O.Hall, W. P. Kegelmeyer, \"SMOTE: synthetic minority over-sampling technique,\" Journal of artificial intelligence research, 321-357, 2002.\n",
    "        - Since class '1' has only 1% in training data and no presence in test set. \n",
    "            - We will ignore prediction of this class. \n",
    "            - With only 8 points our performance metrics will have, very little to NO significance. \n",
    "        - Class 4 has no instances in both train and test set, this class will be implicitly ignored.\n",
    "        - Both these classes will require more data.\n",
    "\n",
    "\n",
    "3. Missing Values\n",
    "    - 9 attributes in the training set have <b> NO MISSING VALUES </b>.\n",
    "    - Continous attributes [carbon, hardness, strength, thickm width, len] don't have any missing values in the train set.\n",
    "        - But our pipeline will still have \"impute with mean step\" for these attributes to deal missingness during inference.\n",
    "    - Out of 38 attributes 29 attributes have missing values. All of which are categorical or ordinal.\n",
    "    - The amount of missingness varies from 8 % to 100%.\n",
    "    - The data skewed in terms of missingness also, such that there are 4 variables \n",
    "        - ['steel', 'surface_quality', 'condition', 'formability'] with [8%, 27%, 33%, 35.4%] missingness, respectively.\n",
    "        - Rest of the missing attributes have median missingness of 98% and a minimum of 76% missingness. \n",
    "    - To deal with this missing-ness we ran various experiments in the background.\n",
    "        - Experiment 1 \n",
    "            - We drop Drop all attributes/columns which have more that 35% missingness.\n",
    "            - This leaves us with only 13 attributes, which is 34% of the original number attributes.\n",
    "            - Imputation with mode of the training data, as all of them are categorical/ordinal.\n",
    "       - Experiment 2\n",
    "           - This experiment was insipired by 2 facts\n",
    "               1. In experiment 1 we had dropped 66% of the attributes. Which is way too many dropped attributes, 25 in number. \n",
    "                   - There is a high chance that some of these attributes have high discrimatory power, wrt to the target.\n",
    "               2. As mentioned above in the data documentation that:\n",
    "                   - '-' represents the not_applicable values\n",
    "                   - Of the 29 categorical variables, 19 have binary values.\n",
    "                   - Except \"shape\", all other 18 attributes have very high missing values.\n",
    "                   - Combining the above facts, it would makes a lot of sense to\n",
    "                       - Impute missing values with  \"not_applicable\".\n",
    "                   - However, we still drop attributes with more than 99% of missing values, which would be only 10 attributes.\n",
    "                3. We would continue to impute ['steel', 'surface_quality', 'condition', 'formability'] with training data's mode in this experiment.\n",
    "       - Experiment 3\n",
    "           - Same as experiment 2, but we also impute ['steel', 'surface_quality', 'condition', 'formability'] with \"not_applicable\".\n",
    "      \n",
    "    \n",
    "4. Since the client is interested in a high True Positivity Rate, we perform grid search based using F_beta, with more importace to recall, i.e. having the beta value set to 1.28. As per documentation of sklearn, a beta value higher than 1.0 prefers recall more than precision. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from statistics import variance, mean\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, classification_report, make_scorer, precision_score, fbeta_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import check_scoring\n",
    "from sklearn.model_selection import cross_validate\n",
    "from collections import defaultdict\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import _safe_indexing, check_random_state, shuffle\n",
    "from scipy import sparse\n",
    "from sklearn.ensemble._forest import ForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from collections import Counter\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pre-processing applied (20 marks)\n",
    "Enter the code in the cells below to execute each of the stated sub-tasks. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since the algorithms used in sklearn provide the functionality to internally convert string based categorical target to appropriate label encoding, we skip this step\n",
    "#### Instead we present here the code to read the data file and get the train, val and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_train_data():\n",
    "    \n",
    "    df = pd.read_csv(\"dataset/anneal.data\")\n",
    "    df = shuffle(df)\n",
    "    \n",
    "    # Because replace \"?\" represents missing. \n",
    "    df.replace(\"?\", np.nan, inplace=True)\n",
    "\n",
    "    # Because only 8 data poins have target as \"1\".\n",
    "    # Our metrics won't be reliable for this class.\n",
    "    \n",
    "    df = df[df.target != \"1\"]\n",
    "    \n",
    "    # Just a fix as some values which are supposed to be int have string representation for this column\n",
    "    df.enamelability = df.enamelability.astype(float)\n",
    "    \n",
    "    \n",
    "    df_y = df.target\n",
    "    df_X = df.drop(labels=[\"target\"], axis=1)\n",
    "    \n",
    "    # Just re-organinsing the data such that countinous columns are at the end. \n",
    "    numerical_features = [\"carbon\", \"hardness\", \"strength\", \"thick\", \"width\", \"len\"]\n",
    "    all_categorical_features = list(set(df_X.columns.to_list()) - set(numerical_features))\n",
    "    df_X = pd.concat([df_X[all_categorical_features], df_X[numerical_features]], axis=1)\n",
    "    \n",
    "    return df_X, df_y\n",
    "\n",
    "def get_test_data():\n",
    "    \n",
    "    df_test = pd.read_csv(\"dataset/anneal.test\")\n",
    "    df_test.replace(\"?\", np.nan, inplace=True)\n",
    "    \n",
    "    df_test.enamelability = df_test.enamelability.astype(float)\n",
    "\n",
    "\n",
    "    y_test = df_test.target\n",
    "    X_test = df_test.drop(labels=[\"target\"], axis=1)\n",
    "\n",
    "    return X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  b) Removing synonymous and noisy attributes if necessary \n",
    "\n",
    "##### All the attributes to remove will be added to a drop_features list\n",
    "##### We will maintain different lists of drop attributes for various experiments as mentioned in the Case Study Section\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = get_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C    790\n",
       "Name: product_type, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Product Type attribute is the same throughout the dataset, hence it can removed\n",
    "X_train.product_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "corr                      100.000000\n",
       "m                         100.000000\n",
       "s                         100.000000\n",
       "exptl                     100.000000\n",
       "jurofm                    100.000000\n",
       "p                         100.000000\n",
       "marvi                     100.000000\n",
       "bc                         99.873418\n",
       "blue_bright_varn_clean     99.493671\n",
       "phos                       99.113924\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display amount of missingness in attributes\n",
    "missing_means = X_train.isnull().mean().multiply(100).sort_values(ascending=False)\n",
    "missing_means.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1 drop all attributes with more the 35% missing values\n",
    "drop_attributes_exp1 = [\"product_type\"] + missing_means[missing_means > 35.0].index.to_list()\n",
    "\n",
    "# Experiment 2 drop all attributes with more the 99% missing values\n",
    "drop_attributes_exp2 = [\"product_type\"] + missing_means[missing_means > 99.0].index.to_list()\n",
    "\n",
    "# Experiment 3; same as Exp 2.\n",
    "drop_attributes_exp3 = drop_attributes_exp2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Column Dropper Transformer for Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnDropperTransformer:\n",
    "    def __init__(self, column):\n",
    "        self.columns = column\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_new = X.drop(self.columns, axis=1)\n",
    "        return X_new\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  c) Dealing with missing values if necessary \n",
    "##### We are dealing with missing values in three different ways as mentioned in Experiment 1, 2 and 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1: Impute missingness less than 35% with mode.\n",
    "mode_imputer_list_exp1 = [\"steel\", \"shape\", \"bore\", \"surface_quality\", \"formability\"]\n",
    "\n",
    "# Experiment 2: Impute missingness >75% with a not_applicable value/category. Impute missingness less than 35% with mode.\n",
    "mode_imputer_list_exp2 = [\"steel\", \"shape\", \"bore\", \"surface_quality\", \"formability\"]\n",
    "na_imputer_list_exp2 = missing_means[(missing_means > 70.0) & (missing_means < 99.0)].index.to_list()\n",
    "\n",
    "# Experiment 3: Impute all missing value with NA category.\n",
    "na_imputer_list_exp3 = missing_means[(missing_means > 0.0) & (missing_means < 99.0)].index.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### NA imputer Transformer for Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NAColumnTransformer:\n",
    "    def __init__(self, column):\n",
    "        self.columns = column\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        without_ordinals = list(set(self.columns) - {'formability', 'enamelability'})\n",
    "        \n",
    "        # Imputer string based features with string NA\n",
    "        X_new = X[without_ordinals].fillna(\"NA\")\n",
    "        X[without_ordinals] = X_new\n",
    "        \n",
    "        # Creating a new 0 category for formability\n",
    "        # Creating a new 0 category for enamelability\n",
    "        if \"formability\" in self.columns:\n",
    "            X['formability'] = X['formability'].fillna(0).astype(int)\n",
    "\n",
    "        if 'enamelability' in self.columns:\n",
    "            X['enamelability'] = X['enamelability'].fillna(0).astype(int)\n",
    "        return X\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  d) Rescaling if necessary\n",
    "### For Random Forest Scale is kept False as it can handle unscaled values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numerical_pipeline(scale=False):\n",
    "    if scale:\n",
    "        numerical_pipeline = Pipeline(steps=[('ss', StandardScaler())])\n",
    "        return numerical_pipeline\n",
    "\n",
    "    return 'passthrough'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) Categorical Feature Pipeline\n",
    "\n",
    "\n",
    "- in case of experiment 3, simple imputer will not have any effect\n",
    "- as all the `nan` values would be imputed by na_imputer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categorical_pipeline(cat_features_to_drop, na_imputer_cols):\n",
    "\n",
    "    categorical_pipeline = Pipeline(steps=[\n",
    "        ('drop_column', ColumnDropperTransformer(cat_features_to_drop)),\n",
    "        ('na_imputer', NAColumnTransformer(na_imputer_cols)),\n",
    "        ('mode', SimpleImputer(strategy='most_frequent')),\n",
    "        ('one-hot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "    ])\n",
    "    return categorical_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f)  Full Preprocessor Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_processeror(all_categorical_features, cat_features_to_drop, numerical_features, na_imputer_cols, scale_numerical):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param all_categorical_features: List of all the categorical features.\n",
    "    :param cat_features_to_drop: List of categorical features to drop.\n",
    "    :param numerical_features: List of numerical features.\n",
    "    :param na_imputer_cols: List of features to imputer with new \"not_applicable\" category. \n",
    "    :param scale_numerical: Boolean which controls scaling of numerical features.\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    \n",
    "    categorical_pipeline = get_categorical_pipeline(cat_features_to_drop, na_imputer_cols)\n",
    "    numerical_pipeline = get_numerical_pipeline(scale=scale_numerical)\n",
    "    full_processor = ColumnTransformer(transformers=[\n",
    "        ('category', categorical_pipeline, all_categorical_features),\n",
    "        ('numerical', numerical_pipeline, numerical_features)\n",
    "    ])\n",
    "    return full_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = X_train.columns.to_list()\n",
    "numerical_features = [\"carbon\", \"hardness\", \"strength\", \"thick\", \"width\", \"len\"]\n",
    "all_categorical_features = list(set(all_features) - set(numerical_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 1 Data Pipeline\n",
    "- We drop Drop all attributes/columns which have more that 35% missingness.\n",
    "- We impute categorical values with mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiment1_pipeline(scale_numerical_features=False):\n",
    "    experiment1_data_pipeline = get_full_processeror(\n",
    "        all_categorical_features=all_categorical_features, \n",
    "        cat_features_to_drop=drop_attributes_exp1, \n",
    "        numerical_features=numerical_features, \n",
    "        na_imputer_cols=[], \n",
    "        scale_numerical=scale_numerical_features)\n",
    "    return experiment1_data_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 2 Data Pipeline\n",
    "- We drop Drop all attributes/columns which have more that 99% missingness.\n",
    "- We impute [\"steel\", \"shape\", \"bore\", \"surface_quality\", \"formability\"] categorical attributes with mode.\n",
    "- Other categorical attributes are imputed with \"NA\". Explaination is given in the case study section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiment2_pipeline(scale_numerical_features=False):\n",
    "    experiment2_data_pipeline = get_full_processeror(\n",
    "        all_categorical_features=all_categorical_features, \n",
    "        cat_features_to_drop=drop_attributes_exp3, \n",
    "        numerical_features=numerical_features, \n",
    "        na_imputer_cols=na_imputer_list_exp2, \n",
    "        scale_numerical=scale_numerical_features)\n",
    "    return experiment2_data_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 3 Data Pipeline\n",
    "- We drop Drop all attributes/columns which have more that 99% missingness.\n",
    "- All categorical attributes are imputed with \"NA\". Explaination is given in the case study section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiment3_pipeline(scale_numerical_features=False):\n",
    "    experiment3_data_pipeline = get_full_processeror(\n",
    "        all_categorical_features=all_categorical_features, \n",
    "        cat_features_to_drop=drop_attributes_exp3, \n",
    "        numerical_features=numerical_features, \n",
    "        na_imputer_cols=na_imputer_list_exp3, \n",
    "        scale_numerical=scale_numerical_features)\n",
    "    return experiment3_data_pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Technique 1 (20 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Discuss your motivation for choosing the technique and provide a schematic figure of the process\n",
    "\n",
    "We use Random Forest as first technique because of the following motivations:\n",
    "\n",
    "1. The algorithm reduces the overfitting and variance problem by using bagging and ensembling.\n",
    "    - It tackles overfitting and variance by builds many decision trees with subsets of features[bagging].\n",
    "    - It then takes the majority vote of the outputs of all trees to give the final output. \n",
    "2. Reqiures no features scaling. \n",
    "3. Robustness towards outliers.\n",
    "4. The grid search is easy to perform.\n",
    "    - Unlike paramteric models, doesn't require careful tuning of regularisation parameter, as regularisation is in-built because of ensembling and bagging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Oversampling\n",
    "As mentioned in the case study section, the target distribution is heavily skewed. \n",
    "To tackle this we employ SMOTE based oversampling.\n",
    "The following code is insipired by:\n",
    "- The SMOTENC class in [imbalance-learn](https://github.com/scikit-learn-contrib/imbalanced-learn/blob/6176807c9c5d68126a79771b6c0fce329f632d2f/imblearn/over_sampling/_smote/base.py#L368) library \n",
    "- N. V. Chawla, K. W. Bowyer, L. O.Hall, W. P. Kegelmeyer, \"SMOTE: synthetic minority over-sampling technique,\" Journal of artificial intelligence research, 321-357, 2002."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Oversample:\n",
    "    def __init__(self, continuous_features_indices, sampling_strategy):\n",
    "        self.continuous_features_ = continuous_features_indices\n",
    "        self.sampling_strategy_ = sampling_strategy\n",
    "\n",
    "    def resample(self, X, y=None):\n",
    "        self.n_features_ = X.shape[1]\n",
    "        target_stats = Counter(y)\n",
    "        class_minority = min(target_stats, key=target_stats.get)\n",
    "        X_continuous = X[:, self.continuous_features_]\n",
    "        X_minority = _safe_indexing(X_continuous, np.flatnonzero(y == class_minority))\n",
    "\n",
    "        self.categorical_features_ = [i for i in range(len(self.continuous_features_), self.n_features_)]\n",
    "        X_categorical = X[:, self.categorical_features_]\n",
    "        self.ohe_ = OneHotEncoder(handle_unknown=\"ignore\", dtype=np.float64, sparse=False)\n",
    "        X_ohe = self.ohe_.fit_transform(X_categorical)\n",
    "\n",
    "\n",
    "        var = X_minority.var(axis=0)\n",
    "        median_std_ = np.median(np.sqrt(var))\n",
    "        # Categorical attributes\n",
    "\n",
    "        X_encoded_pre = np.hstack((X_continuous, X_ohe))\n",
    "\n",
    "        X_resampled = [X_encoded_pre]\n",
    "        y_resampled = [y.copy()]\n",
    "\n",
    "        X_categorical_transformed = X_ohe * median_std_ / 2\n",
    "\n",
    "        X_encoded = np.hstack((X_continuous, X_categorical_transformed))\n",
    "\n",
    "        for class_sample, n_samples in self.sampling_strategy_.items():  \n",
    "            target_class_indices = np.flatnonzero(y == class_sample)\n",
    "            X_class = _safe_indexing(X_encoded, target_class_indices)\n",
    "            nn = NearestNeighbors(n_neighbors=6)\n",
    "            # print(\"looped\")\n",
    "\n",
    "            nn.fit(sparse.csr_matrix(X_class))\n",
    "            nns = nn.kneighbors(sparse.csr_matrix(X_class), return_distance=False)[:, 1:]\n",
    "\n",
    "            n_samples_gen = n_samples-X_class.shape[0]\n",
    "            X_new, y_new = self.gen_data(X_class, y.dtype, class_sample, nns, n_samples_gen)\n",
    "            X_resampled.append(X_new)\n",
    "            y_resampled.append(y_new)\n",
    "\n",
    "        X_resampled = np.vstack(X_resampled)\n",
    "        y_resampled = np.hstack(y_resampled)\n",
    "\n",
    "        # reverse the encoding of the categorical features\n",
    "        X_res_cat = X_resampled[:, len(self.continuous_features_):]\n",
    "        X_res_cat_dec = self.ohe_.inverse_transform(X_res_cat)\n",
    "\n",
    "        X_resampled = np.hstack(\n",
    "            (\n",
    "                X_resampled[:, : len(self.continuous_features_)],\n",
    "                X_res_cat_dec,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return X_resampled, y_resampled\n",
    "\n",
    "    def gen_data(self, X, y_dtype, y_type, nearest_neighours, n_samples):\n",
    "        random_state = check_random_state(None)\n",
    "        samples_indices = random_state.randint(low=0, high=nearest_neighours.size, size=n_samples)\n",
    "        rows = np.floor_divide(samples_indices, nearest_neighours.shape[1])\n",
    "        cols = np.mod(samples_indices, nearest_neighours.shape[1])\n",
    "\n",
    "        diffs = X[nearest_neighours[rows, cols]] - X[rows]  \n",
    "        X_new = X[rows] + random_state.uniform(size=n_samples)[:, np.newaxis] * diffs\n",
    "\n",
    "        all_neighbors = X[nearest_neighours[rows]] \n",
    "\n",
    "        categories_size = [len(self.continuous_features_)] + [cat.size for cat in self.ohe_.categories_]\n",
    "\n",
    "        for start_idx, end_idx in zip(\n",
    "                np.cumsum(categories_size)[:-1], np.cumsum(categories_size)[1:]\n",
    "        ):\n",
    "            col_maxs = all_neighbors[:, :, start_idx:end_idx].sum(axis=1)\n",
    "            # tie breaking argmax\n",
    "            is_max = np.isclose(col_maxs, col_maxs.max(axis=1, keepdims=True))\n",
    "            max_idxs = random_state.permutation(np.argwhere(is_max))\n",
    "            xs, idx_sels = np.unique(max_idxs[:, 0], return_index=True)\n",
    "            col_sels = max_idxs[idx_sels, 1]\n",
    "            ys = start_idx + col_sels\n",
    "            X_new[:, start_idx:end_idx] = 0\n",
    "            X_new[xs, ys] = 1\n",
    "\n",
    "        y_new = np.full(n_samples, fill_value=y_type, dtype=y_dtype)\n",
    "        return X_new, y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "numericals_indices = [i for i in range(6)]\n",
    "oversample = Oversample(continuous_features_indices=numericals_indices,\n",
    "                    sampling_strategy={'2': 100, '5': 100, 'U': 120})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We slightly modify Random Forest and redefine the fit method, such that before calling fit we over sample the data.\n",
    "This ensures that only the train set it oversamples, as test set set is only fed to the predict method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestClassifierCustom(ForestClassifier):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_estimators=100,\n",
    "            *,\n",
    "            criterion=\"gini\",\n",
    "            max_depth=None,\n",
    "            min_samples_split=2,\n",
    "            min_samples_leaf=1,\n",
    "            min_weight_fraction_leaf=0.0,\n",
    "            max_features=\"sqrt\",\n",
    "            max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0,\n",
    "            bootstrap=True,\n",
    "            oob_score=False,\n",
    "            n_jobs=None,\n",
    "            random_state=None,\n",
    "            verbose=0,\n",
    "            warm_start=False,\n",
    "            class_weight=None,\n",
    "            ccp_alpha=0.0,\n",
    "            max_samples=None,\n",
    "            oversample=None\n",
    "    ):\n",
    "        super().__init__(\n",
    "            base_estimator=DecisionTreeClassifier(),\n",
    "            n_estimators=n_estimators,\n",
    "            estimator_params=(\n",
    "                \"criterion\",\n",
    "                \"max_depth\",\n",
    "                \"min_samples_split\",\n",
    "                \"min_samples_leaf\",\n",
    "                \"min_weight_fraction_leaf\",\n",
    "                \"max_features\",\n",
    "                \"max_leaf_nodes\",\n",
    "                \"min_impurity_decrease\",\n",
    "                \"random_state\",\n",
    "                \"ccp_alpha\",\n",
    "            ),\n",
    "            bootstrap=bootstrap,\n",
    "            oob_score=oob_score,\n",
    "            n_jobs=n_jobs,\n",
    "            random_state=random_state,\n",
    "            verbose=verbose,\n",
    "            warm_start=warm_start,\n",
    "            class_weight=class_weight,\n",
    "            max_samples=max_samples,\n",
    "        )\n",
    "\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.min_weight_fraction_leaf = min_weight_fraction_leaf\n",
    "        self.max_features = max_features\n",
    "        self.max_leaf_nodes = max_leaf_nodes\n",
    "        self.min_impurity_decrease = min_impurity_decrease\n",
    "        self.ccp_alpha = ccp_alpha\n",
    "        self.oversample = oversample\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        X, y = self.oversample.resample(X, y)\n",
    "        super().fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter the correct code in the cells below to execute each of the stated sub-tasks.\n",
    "### b) Setting hyper parameters with rationale\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We perform a grid search on number of estimators (decision trees) and max depth of those decision trees.\n",
    "###### The number of estimators will tackle variance problem and but at the same time will increase the time complexity of the algorithm.\n",
    "###### We tune the max depth parameter casue very deep descision trees cause overfitting, but literature also suggests that, we can allow the decision trees to grow as deep as possible as long as the ensemble is large enough.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_param_grid = {\n",
    "        \"model__n_estimators\": [20, 40, 60, 80, 100, 120, 150, 200],\n",
    "        \"model__max_depth\": [5, 10, 20, 30, 40, None],\n",
    "\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Performance metrics for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_biased_Fbeta(y_true, y_pred):\n",
    "    fbs = fbeta_score(y_true, y_pred, average='macro', beta=1.28)\n",
    "    return fbs\n",
    "\n",
    "def print_recall_biased_Fbeta(y_true, y_pred):\n",
    "    print(classification_report(y_true, y_pred, zero_division=0))  # print classification report\n",
    "    fbs = fbeta_score(y_true, y_pred, average='macro', beta=1.28)\n",
    "    print(\"F1_beta: \", fbs)\n",
    "    print(\"\\n***---***\\n\")\n",
    "    return fbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Optimising hyper parameters\n",
    "We perform nested cross validation along with grid search, and return the best estimators. \n",
    "Instead of running nested CV only once, we perform many trials of it and record the best estimators on each fold, this is because the \n",
    "during nested CV the best estimator on each fold might be different, so we keep track of the best estimators during many runs and pick the \n",
    "top 3 estimators by frequency. \n",
    "\n",
    "These top 3 estimators will then be ensembled again to get the final classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_cross_validation(X_train, y_train, param_grid, classifier, datapipeline, rf_or_lr):\n",
    "    \"\"\"\n",
    "    X_train, y_train: Training Data\n",
    "    param_grid: parameter grid\n",
    "    classifier: classifier used.\n",
    "    datapipeline: datapipeline of the experiment\n",
    "    rf_or_lr: is the classifier RandomForest or Linear Regression. Helps in recoding the winning parameters.\n",
    "    \"\"\"\n",
    "    n_jobs = 40\n",
    "\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocess', datapipeline),\n",
    "        ('model', classifier)\n",
    "    ])\n",
    "\n",
    "    clf = GridSearchCV(estimator=pipeline, param_grid=param_grid,\n",
    "                       scoring=make_scorer(recall_biased_Fbeta), cv=3, n_jobs=n_jobs, return_train_score=True)\n",
    "\n",
    "    \n",
    "    scorer = check_scoring(clf, scoring=make_scorer(print_recall_biased_Fbeta))\n",
    "    cv_results = cross_validate(\n",
    "        estimator=clf,\n",
    "        X=X_train,\n",
    "        y=y_train,\n",
    "        scoring={\"score\": scorer},\n",
    "        cv=3,\n",
    "        n_jobs=n_jobs,\n",
    "        pre_dispatch=\"2*n_jobs\",\n",
    "        error_score=np.nan,\n",
    "        return_estimator=True,\n",
    "        return_train_score=True\n",
    "    )\n",
    "\n",
    "    run_test_Fbeta_score_mean = mean(cv_results['test_score'])\n",
    "    run_train_Fbeta_score_mean = mean(cv_results['train_score'])\n",
    "    \n",
    "    # run_var = variance(cv_results['test_score'])\n",
    "    mean_train_Fbeta_scores.append(run_train_Fbeta_score_mean)\n",
    "    mean_test_Fbeta_scores.append(run_test_Fbeta_score_mean)\n",
    "    \n",
    "    for estimator in cv_results['estimator']:\n",
    "        if rf_or_lr == 'rf':\n",
    "            estimator_frequency[(estimator.best_estimator_.named_steps.model.max_depth, estimator.best_estimator_.named_steps.model.n_estimators)] += 1\n",
    "        elif rf_or_lr == 'lr':\n",
    "            estimator_frequency[(estimator.best_estimator_.named_steps.model.penalty, estimator.best_estimator_.named_steps.model.C)] += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEAN TRAIN FBeta:  0.994385574519773 MEAN TEST FBeta:  0.9032225328187478\n",
      "TOP 3 estimators [0th index is max_depth and 1st index is n_estimators]\n",
      "[('l2', 10), ('l1', 15), ('l1', 10)]\n"
     ]
    }
   ],
   "source": [
    "mean_train_Fbeta_scores = []\n",
    "mean_test_Fbeta_scores = []\n",
    "estimator_frequency = defaultdict(int)\n",
    "rf = RandomForestClassifierCustom(oversample=oversample, n_jobs=40, oob_score=False)\n",
    "exp1_pipeline = get_experiment1_pipeline()\n",
    "for i in range(5):\n",
    "    nested_cross_validation(X_train.copy(), y_train.copy(), rf_param_grid, rf, exp1_pipeline, rf_or_lr='rf')\n",
    "print(\"MEAN TRAIN FBeta: \", mean(mean_train_Fbeta_scores), \"MEAN TEST FBeta: \", mean(mean_test_Fbeta_scores))\n",
    "rf_experiment1_best_estimators = [x[0] for x in list(sorted(estimator_frequency.items(), key=lambda item: item[1], reverse=True))[:3]]\n",
    "print(\"TOP 3 estimators [0th index is max_depth and 1st index is n_estimators]\")\n",
    "print(rf_experiment1_best_estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEAN TRAIN FBeta:  0.9979216619053568 MEAN TEST FBeta:  0.9399244615810068\n",
      "TOP 3 estimators [0th index is max_depth and 1st index is n_estimators]\n",
      "[(20, 20), (40, 150), (40, 40)]\n"
     ]
    }
   ],
   "source": [
    "mean_train_Fbeta_scores = []\n",
    "mean_test_Fbeta_scores = []\n",
    "estimator_frequency = defaultdict(int)\n",
    "rf = RandomForestClassifierCustom(oversample=oversample, n_jobs=40, oob_score=False)\n",
    "exp2_pipeline = get_experiment2_pipeline()\n",
    "for i in range(5):\n",
    "    nested_cross_validation(X_train.copy(), y_train.copy(), rf_param_grid, rf, exp2_pipeline, rf_or_lr='rf')\n",
    "print(\"MEAN TRAIN FBeta: \", mean(mean_train_Fbeta_scores), \"MEAN TEST FBeta: \", mean(mean_test_Fbeta_scores))\n",
    "rf_experiment2_best_estimators = [x[0] for x in list(sorted(estimator_frequency.items(), key=lambda item: item[1], reverse=True))[:3]]\n",
    "print(\"TOP 3 estimators [0th index is max_depth and 1st index is n_estimators]\")\n",
    "print(rf_experiment2_best_estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEAN TRAIN FBeta:  1.0 MEAN TEST FBeta:  0.9827723978919465\n",
      "TOP 3 estimators [0th index is max_depth and 1st index is n_estimators]\n",
      "[(20, 60), (10, 40), (40, 150)]\n"
     ]
    }
   ],
   "source": [
    "mean_train_Fbeta_scores = []\n",
    "mean_test_Fbeta_scores = []\n",
    "estimator_frequency = defaultdict(int)\n",
    "rf = RandomForestClassifierCustom(oversample=oversample, n_jobs=40, oob_score=False)\n",
    "exp3_pipeline = get_experiment3_pipeline()\n",
    "for i in range(5):\n",
    "    nested_cross_validation(X_train.copy(), y_train.copy(), rf_param_grid, rf, exp3_pipeline, rf_or_lr='rf')\n",
    "print(\"MEAN TRAIN FBeta: \", mean(mean_train_Fbeta_scores), \"MEAN TEST FBeta: \", mean(mean_test_Fbeta_scores))\n",
    "rf_experiment3_best_estimators = [x[0] for x in list(sorted(estimator_frequency.items(), key=lambda item: item[1], reverse=True))[:3]]\n",
    "print(\"TOP 3 estimators [0th index is max_depth and 1st index is n_estimators]\")\n",
    "print(rf_experiment3_best_estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance on Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data pipeline from experiment 3 shows the best result in cross validation, hence we should reccomend this model pipeline.\n",
    "#### We still call the `rf_final_results` on all the experiment pipelines, just for comapisons purpose.\n",
    "#### Re-fitting  Ensembling top 3 random forest reported by nested cross validation and calculating scores on Test Set\n",
    "\n",
    "During nested cross validation, different models/parameters win on different folds of the data.\n",
    "So, in the above 10 trials of nested cross validation we kept track of the winning parameters for Random Forest\n",
    "and now we make an ensemble of the top three random forests and caluculate the scores on the test set to show the finals results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_final_results(pipeline_method, max_depth_n_est_list):\n",
    "    \"\"\"\n",
    "    pipeline_method: the method to get pipeline of the experiment. eg: get_experiment2_pipeline\n",
    "    max_depth_n_est_list: list of top 3 estimators from nested cv. The first parameter is \n",
    "    \"\"\"\n",
    "    n_jobs = 40\n",
    "    numericals_indices = [i for i in range(6)]\n",
    "    oversample_final = Oversample(continuous_features_indices=numericals_indices,\n",
    "                                  sampling_strategy={0: 100, 2: 100, 3: 120})\n",
    "    rf1 = RandomForestClassifierCustom(oversample=oversample_final, max_depth=max_depth_n_est_list[0][0], n_estimators=max_depth_n_est_list[0][1], n_jobs=n_jobs,\n",
    "                                       oob_score=False)\n",
    "    rf2 = RandomForestClassifierCustom(oversample=oversample_final, max_depth=max_depth_n_est_list[1][0], n_estimators=max_depth_n_est_list[1][1], n_jobs=n_jobs,\n",
    "                                       oob_score=False)\n",
    "    rf3 = RandomForestClassifierCustom(oversample=oversample_final, max_depth=max_depth_n_est_list[2][0], n_estimators=max_depth_n_est_list[2][1], n_jobs=n_jobs,\n",
    "                                       oob_score=False)\n",
    "    clfHard = VotingClassifier(estimators=[('rf1', rf1), ('rf2', rf2), ('rf3', rf3)], voting='hard')\n",
    "    X_test, y_test = get_test_data()\n",
    "    final_data_pipeline = pipeline_method()\n",
    "    rf_pipeline = Pipeline(steps=[\n",
    "        ('preprocess', final_data_pipeline),\n",
    "        ('model', clfHard)\n",
    "    ])\n",
    "    rf_pipeline.fit(X_train, y_train)\n",
    "    y_pred = rf_pipeline.predict(X_test)\n",
    "    print(\"RF TEST SET REPORT\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "    print(\"-\"*60)\n",
    "    print(\"RF TRAIN SET REPORT\")\n",
    "    y_train_pred = lr_pipeline.predict(X_train)\n",
    "    print(classification_report(y_train, y_train_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results from Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF TEST SET REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       0.79      1.00      0.88        11\n",
      "           3       1.00      0.95      0.97        76\n",
      "           5       0.88      1.00      0.93         7\n",
      "           U       1.00      1.00      1.00         6\n",
      "\n",
      "    accuracy                           0.96       100\n",
      "   macro avg       0.92      0.99      0.95       100\n",
      "weighted avg       0.97      0.96      0.96       100\n",
      "\n",
      "------------------------------------------------------------\n",
      "RF TRAIN SET REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       0.97      0.97      0.97        88\n",
      "           3       0.99      1.00      0.99       608\n",
      "           5       1.00      1.00      1.00        60\n",
      "           U       1.00      0.94      0.97        34\n",
      "\n",
      "    accuracy                           0.99       790\n",
      "   macro avg       0.99      0.98      0.98       790\n",
      "weighted avg       0.99      0.99      0.99       790\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_final_results(pipeline_method=get_experiment1_pipeline, max_depth_n_est_list=rf_experiment1_best_estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results from Experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF TEST SET REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       0.92      1.00      0.96        11\n",
      "           3       1.00      0.99      0.99        76\n",
      "           5       1.00      1.00      1.00         7\n",
      "           U       1.00      1.00      1.00         6\n",
      "\n",
      "    accuracy                           0.99       100\n",
      "   macro avg       0.98      1.00      0.99       100\n",
      "weighted avg       0.99      0.99      0.99       100\n",
      "\n",
      "------------------------------------------------------------\n",
      "RF TRAIN SET REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       0.97      0.97      0.97        88\n",
      "           3       0.99      1.00      0.99       608\n",
      "           5       1.00      1.00      1.00        60\n",
      "           U       1.00      0.94      0.97        34\n",
      "\n",
      "    accuracy                           0.99       790\n",
      "   macro avg       0.99      0.98      0.98       790\n",
      "weighted avg       0.99      0.99      0.99       790\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_final_results(pipeline_method=get_experiment2_pipeline, max_depth_n_est_list=rf_experiment2_best_estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results from Experiment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF TEST SET REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       1.00      1.00      1.00        11\n",
      "           3       1.00      1.00      1.00        76\n",
      "           5       1.00      1.00      1.00         7\n",
      "           U       1.00      1.00      1.00         6\n",
      "\n",
      "    accuracy                           1.00       100\n",
      "   macro avg       1.00      1.00      1.00       100\n",
      "weighted avg       1.00      1.00      1.00       100\n",
      "\n",
      "------------------------------------------------------------\n",
      "RF TRAIN SET REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       0.97      0.97      0.97        88\n",
      "           3       0.99      1.00      0.99       608\n",
      "           5       1.00      1.00      1.00        60\n",
      "           U       1.00      0.94      0.97        34\n",
      "\n",
      "    accuracy                           0.99       790\n",
      "   macro avg       0.99      0.98      0.98       790\n",
      "weighted avg       0.99      0.99      0.99       790\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_final_results(pipeline_method=get_experiment3_pipeline, max_depth_n_est_list=rf_experiment3_best_estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Technique 2 (20 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Discuss your motivation for choosing the technique and  provide a schematic figure of the process\n",
    "\n",
    "Second we choose Logistic Regression without feature mapping. Logistic Regression is that it is one the simplest parametric model for classification and using it without feature mapping further simplifies the model. Primary reason of choosing this combination is applying the simplest hypothesis to the data. (Occam's Razor)\n",
    "\n",
    "We use `liblinear` as the solver because it allows both `l1` and `l2` penalty terms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We overload the Logistic regression class to oversample while training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "numericals_indices = [i for i in range(6)]\n",
    "oversample_lr = Oversample(continuous_features_indices=numericals_indices,\n",
    "                    sampling_strategy={'2': 150, '5': 120, 'U': 100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionCustom(LogisticRegression):\n",
    "\n",
    "    def __init__(self, oversample, solver, penalty=None, C=None):\n",
    "        super().__init__(penalty=penalty, C=C, solver=solver)\n",
    "        self.oversample = oversample\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        X, y = self.oversample.resample(X, y)\n",
    "        super().fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Setting hyper parameters with rationale\n",
    "\n",
    "For Logistic Regression we intend on tuning the penalty type, 'l1' and 'l2' and these have direct relation with the final weights/paramters that are learnt by the model.\n",
    "- L1: Produces sparse weights.\n",
    "- L2: Doesn't push less important features to 0.\n",
    "\n",
    "Second we tune the C parameter, which controls regularisation strength. \n",
    "- Smaller values of C specify stronger regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_param_grid = {\n",
    "        \"model__penalty\": ['l1', 'l2'],\n",
    "        \"model__C\": [0.5, 1, 5, 10, 15, 20],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Optimising hyper parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEAN TRAIN FBeta:  0.6823245534669126 MEAN TEST FBeta:  0.6359302379085366\n",
      "TOP 3 estimators [0th index is max_depth and 1st index is n_estimators]\n",
      "[('l2', 15), ('l1', 15), ('l2', 20)]\n"
     ]
    }
   ],
   "source": [
    "mean_train_Fbeta_scores = []\n",
    "mean_test_Fbeta_scores = []\n",
    "estimator_frequency = defaultdict(int)\n",
    "lr = LogisticRegressionCustom(oversample=oversample_lr, solver='liblinear')\n",
    "exp1_pipeline = get_experiment1_pipeline(scale_numerical_features=True)\n",
    "for i in range(5):\n",
    "    nested_cross_validation(X_train.copy(), y_train.copy(), lr_param_grid, lr, exp1_pipeline, rf_or_lr='lr')\n",
    "print(\"MEAN TRAIN FBeta: \", mean(mean_train_Fbeta_scores), \"MEAN TEST FBeta: \", mean(mean_test_Fbeta_scores))\n",
    "experiment1_best_estimators = [x[0] for x in list(sorted(estimator_frequency.items(), key=lambda item: item[1], reverse=True))[:3]]\n",
    "print(\"TOP 3 estimators [0th index is max_depth and 1st index is n_estimators]\")\n",
    "print(experiment1_best_estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEAN TRAIN FBeta:  0.8638307584123754 MEAN TEST FBeta:  0.8294546195945323\n",
      "TOP 3 estimators [0th index is penalty and 1st index is C]\n",
      "[('l1', 20), ('l1', 15), ('l1', 5)]\n"
     ]
    }
   ],
   "source": [
    "mean_train_Fbeta_scores = []\n",
    "mean_test_Fbeta_scores = []\n",
    "estimator_frequency = defaultdict(int)\n",
    "lr = LogisticRegressionCustom(oversample=oversample_lr, solver='liblinear')\n",
    "exp2_pipeline = get_experiment2_pipeline(scale_numerical_features=True)\n",
    "for i in range(5):\n",
    "    nested_cross_validation(X_train.copy(), y_train.copy(), lr_param_grid, lr, exp2_pipeline, rf_or_lr='lr')\n",
    "print(\"MEAN TRAIN FBeta: \", mean(mean_train_Fbeta_scores), \"MEAN TEST FBeta: \", mean(mean_test_Fbeta_scores))\n",
    "experiment2_best_estimators = [x[0] for x in list(sorted(estimator_frequency.items(), key=lambda item: item[1], reverse=True))[:3]]\n",
    "print(\"TOP 3 estimators [0th index is penalty and 1st index is C]\")\n",
    "print(experiment2_best_estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEAN TRAIN FBeta:  0.9774212941057975 MEAN TEST FBeta:  0.954270098434053\n",
      "TOP 3 estimators [0th index is penalty and 1st index is C]\n",
      "[('l1', 20), ('l1', 5), ('l1', 15)]\n"
     ]
    }
   ],
   "source": [
    "mean_train_Fbeta_scores = []\n",
    "mean_test_Fbeta_scores = []\n",
    "estimator_frequency = defaultdict(int)\n",
    "lr = LogisticRegressionCustom(oversample=oversample_lr, solver='liblinear')\n",
    "exp3_pipeline = get_experiment3_pipeline(scale_numerical_features=True)\n",
    "for i in range(5):\n",
    "    nested_cross_validation(X_train.copy(), y_train.copy(), lr_param_grid, lr, exp3_pipeline, rf_or_lr='lr')\n",
    "print(\"MEAN TRAIN FBeta: \", mean(mean_train_Fbeta_scores), \"MEAN TEST FBeta: \", mean(mean_test_Fbeta_scores))\n",
    "experiment3_best_estimators = [x[0] for x in list(sorted(estimator_frequency.items(), key=lambda item: item[1], reverse=True))[:3]]\n",
    "print(\"TOP 3 estimators [0th index is penalty and 1st index is C]\")\n",
    "print(experiment3_best_estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We choose `l1` penalty along with `20` as the `C value` and the `experiment 3's` pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Performance metrics on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LR_test_set_performance(data_pipeline):\n",
    "    lr_final = LogisticRegressionCustom(oversample=oversample_lr, solver='liblinear', penalty='l1', C=20)\n",
    "    X_test, y_test = get_test_data()\n",
    "    final_data_pipeline = get_experiment3_pipeline()\n",
    "    lr_pipeline = Pipeline(steps=[\n",
    "        ('preprocess', data_pipeline),\n",
    "        ('model', lr_final)\n",
    "    ])\n",
    "    lr_pipeline.fit(X_train, y_train)\n",
    "    y_pred = lr_pipeline.predict(X_test)\n",
    "    print(\"LR TEST SET REPORT\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "    print(\"-\"*60)\n",
    "    print(\"LR TRAIN SET REPORT\")\n",
    "    y_train_pred = lr_pipeline.predict(X_train)\n",
    "    print(classification_report(y_train, y_train_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LR Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR TEST SET REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       0.67      0.73      0.70        11\n",
      "           3       0.95      0.92      0.93        76\n",
      "           5       0.78      1.00      0.88         7\n",
      "           U       1.00      0.83      0.91         6\n",
      "\n",
      "    accuracy                           0.90       100\n",
      "   macro avg       0.85      0.87      0.85       100\n",
      "weighted avg       0.91      0.90      0.90       100\n",
      "\n",
      "------------------------------------------------------------\n",
      "LR TRAIN SET REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       0.68      0.44      0.54        88\n",
      "           3       0.89      0.94      0.92       608\n",
      "           5       0.76      0.78      0.77        60\n",
      "           U       0.96      0.74      0.83        34\n",
      "\n",
      "    accuracy                           0.87       790\n",
      "   macro avg       0.82      0.73      0.76       790\n",
      "weighted avg       0.86      0.87      0.86       790\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_exp1_data_pipeline = get_experiment1_pipeline()\n",
    "LR_test_set_performance(lr_exp1_data_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LR Experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR TEST SET REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       0.75      0.82      0.78        11\n",
      "           3       0.97      0.96      0.97        76\n",
      "           5       1.00      1.00      1.00         7\n",
      "           U       1.00      1.00      1.00         6\n",
      "\n",
      "    accuracy                           0.95       100\n",
      "   macro avg       0.93      0.94      0.94       100\n",
      "weighted avg       0.95      0.95      0.95       100\n",
      "\n",
      "------------------------------------------------------------\n",
      "LR TRAIN SET REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       0.73      0.56      0.63        88\n",
      "           3       0.93      0.97      0.95       608\n",
      "           5       1.00      1.00      1.00        60\n",
      "           U       0.97      0.91      0.94        34\n",
      "\n",
      "    accuracy                           0.92       790\n",
      "   macro avg       0.91      0.86      0.88       790\n",
      "weighted avg       0.92      0.92      0.92       790\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_exp2_data_pipeline = get_experiment2_pipeline()\n",
    "LR_test_set_performance(lr_exp2_data_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LR Experiment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR TEST SET REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       0.92      1.00      0.96        11\n",
      "           3       1.00      0.99      0.99        76\n",
      "           5       1.00      1.00      1.00         7\n",
      "           U       1.00      1.00      1.00         6\n",
      "\n",
      "    accuracy                           0.99       100\n",
      "   macro avg       0.98      1.00      0.99       100\n",
      "weighted avg       0.99      0.99      0.99       100\n",
      "\n",
      "------------------------------------------------------------\n",
      "LR TRAIN SET REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       0.97      0.95      0.96        88\n",
      "           3       0.99      1.00      0.99       608\n",
      "           5       1.00      1.00      1.00        60\n",
      "           U       1.00      0.94      0.97        34\n",
      "\n",
      "    accuracy                           0.99       790\n",
      "   macro avg       0.99      0.97      0.98       790\n",
      "weighted avg       0.99      0.99      0.99       790\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_exp3_data_pipeline = get_experiment3_pipeline()\n",
    "LR_test_set_performance(lr_exp3_data_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparison of metrics performance for testing (16 marks) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Use of cross validation for both techniques to deal with over-fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The method `nestes_cross_validation` defined above is the method used to perform Nested Cross Validation.  \n",
    "- Nested cross validation was used to get a robust validation score and get best parameters through grid search.\n",
    "- The challenge of using oversampling along with nested cross validation was dealt with by modifying the fit method by using Overloading of Random Forest and Logistic Regression Classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Comparison with appropriate metrics for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have used classification report which conveys Precision, Recall, F1 and Accuracy.\n",
    "- Such a report conveys all the important metrics about the predicitons of the classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Model selection (ROC or other charts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We used classfication reports, which convery that the Random Forest outperform Logistic Regression on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final recommendation of best model (8 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Discuss the results from a technical perspective, for example, overfitting discussion, complexity and efficiency\n",
    "\n",
    "- Both RF and LR have the similar performance, however Random Forest Outperforms LR.\n",
    "- Since RF correctly predicted all the test set instances, it conveys that the test set has easy instances. \n",
    "    - Also, classes '2', '5', 'U' have only 11, 7 and 6 instances.\n",
    "    - So, the test set metrics might not be so reliable.\n",
    "    - Such good performance could be just luck, small test set size and easy test set.\n",
    "- Overfitting:\n",
    "    - We saw signs of overfitting with the Experiment 1's pipeline. \n",
    "        - We had removed over `60%` of the features in experiment 1.\n",
    "        - MEAN TRAIN FBeta:  `0.994` \n",
    "        - MEAN TEST FBeta:  `0.903`\n",
    "    - Overfitting was reduced when we started using more features in Experiment 2 and Experiment 3.\n",
    "        - Experiment 2\n",
    "            - MEAN TRAIN FBeta:  `0.997` \n",
    "            - MEAN TEST FBeta:  `0.939`\n",
    "        - Experiment 3\n",
    "            - MEAN TRAIN FBeta:  `1.0` \n",
    "            - MEAN TEST FBeta:  `0.982`\n",
    "- Underfitting:\n",
    "    - We saw signs of underfitting when we used LR with the less features in Experiment 1\n",
    "        - MEAN TRAIN FBeta:  0.6823245534669126 \n",
    "        - MEAN TEST FBeta:  0.6359302379085366\n",
    "        - Also, with less features we observed model preferred l2 penalty.\n",
    "- Complexity\n",
    "    - With Random Forest we let the individual decision trees grow as deep as possible which increases complexity but since we use bagging and ensembling along with it, it combats overffitng. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our final recommendation of best model with the given data set would be \n",
    "- An Ensemble of Random Forest along with the data pipeline used in experiment 3, which imputes the missing values with a new category. \n",
    "- And we would conduct the training with over sampling using SMOTE. \n",
    "- We chose RF not only because of it's perfect result but also becuase of the nature of the data and problem, a Rule Based (Decision Tree) based classifier would be appropriate. \n",
    "- Also, Random Forest combats overfitting, outliers better than Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Discuss the results from a business perspective, for example, results interpretation, relevance and balance with technical perspective\n",
    "\n",
    "- The classification report shows that the results on the test set are perfect, but we should take into account that the test was of only 100 instance, where except the majority class, the other classes have very small representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion (8 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) What has been successfully accomplished and what has not been successful?\n",
    "\n",
    "- We have been able to succcesfully train 2 models, Random Forest and Logistic Regression. \n",
    "- We experimented with various imputation methods. \n",
    "- We finally were able to decide on recommending the Random Forest classifier. Because of \n",
    "    - The perfect result on the test set.\n",
    "    - Nature of the problem and the data. \n",
    "- Also our model meets the client expectation of more than 98% accuracy for each class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Reflecting back on the analysis, what could you have done differently if you were to do the project again?\n",
    "\n",
    "- We believe that the test set is too small for any concrete conclusion of the performance of the classifiers.\n",
    "- Also it could be the case the test set might be too easy.\n",
    "- As a recommendation to the firm, I'd ask them to collect more test data, specially corersponding to the non-majority classes.\n",
    "\n",
    "- The data set might have some problem because  to be a problem because \n",
    "\n",
    "While reading https://archive.ics.uci.edu/ml/datasets/Annealing, the data documenation mentions \n",
    "\n",
    "The '-' values are actually 'not_applicable' values rather than 'missing_values' (and so can be treated as legal discrete values rather than as showing the absence of a discrete value).\n",
    "\n",
    "But we don't find any '-' in the data, however, imputing with 'not_applicable' values gave the best results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
