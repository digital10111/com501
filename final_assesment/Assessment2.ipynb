{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science ODL Project: Assessment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ID:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to the brief"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Aims, objectives and plan (4 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Aims and objectives\n",
    "A company has collected various attributes of the steel that they anneal, along with the attributes they have recorded the kind of annealing which was done previouly. \n",
    "They want to use these attributes and predict the type of Annealling that should be performed given a new instance of steel atrtibutes. \n",
    "The aim is develop a unbiased model which correctly predicts the annlealing class 98% of the times regardless of the class. In other words, if there 100 instances of each class, then the model \n",
    "should be able to correctly detect atleast 98 instances correctly from each class, which indicates that they want a model which has a very high True Positivity Rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  b) Plan\n",
    "Please demonstrate how you have conducted the project with a simple Gantt chart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding the case study (4 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Case study analysis\n",
    "State the key points that you found in the case and how you intend to deal with them appropriately to address the client's needs. (You can include more than four points.)\n",
    "\n",
    "\n",
    "1. Overview of the Data\n",
    "- The data is medium size with 798 training examples and 100 examples in test set. \n",
    "- There are 38 attributes, out which 6 are real-valued, 3 are oridnals and 29 categorical attributes. \n",
    "- The documentation of the data states that '-' represents the not_applicable values and '?' represent missing values. \n",
    "- Of the 29 categorical variables, 19 have binary values.\n",
    "- There are 6 documentated annealing (target) types. '1', '2', '3', '4', '5' and 'U' \n",
    "\n",
    "2. Class Imbalance\n",
    "    - Target distribution is heavily disblanced \n",
    "        - Class '3' dominates with 76% of the instances\n",
    "        - Class '2' is present in  11.4% of instances\n",
    "        - Class '5' is present in 7% of the instances\n",
    "        - Class 'U' is present in 4% of the instances\n",
    "        - Class '1' is present in 1% of the instances [only 8 examples]\n",
    "        - Class '4' is not present in the data.\n",
    "        - We will use synthetic minority over-sampling technique on the non-dominant classes.\n",
    "            - N. V. Chawla, K. W. Bowyer, L. O.Hall, W. P. Kegelmeyer, \"SMOTE: synthetic minority over-sampling technique,\" Journal of artificial intelligence research, 321-357, 2002.\n",
    "        - Since class '1' has only 1% in training data and no presence in test set. \n",
    "            - We will ignore prediction of this class. \n",
    "            - With only 8 points our performance metrics will have, very little to NO significance. \n",
    "        - Class 4 has no instances in both train and test set, this class will be implicitly ignored.\n",
    "        - Both these classes will require more data.\n",
    "\n",
    "\n",
    "3. Missing Values\n",
    "    - 9 attributes in the training set have <b> NO MISSING VALUES </b>.\n",
    "    - Continous attributes [carbon, hardness, strength, thickm width, len] don't have any missing values in the train set.\n",
    "        - But our pipeline will still have \"impute with mean step\" for these attributes to deal missingness during inference.\n",
    "    - Out of 38 attributes 29 attributes have missing values. All of which are categorical or ordinal.\n",
    "    - The amount of missingness varies from 8 % to 100%.\n",
    "    - The data skewed in terms of missingness also, such that there are 4 variables \n",
    "        - ['steel', 'surface_quality', 'condition', 'formability'] with [8%, 27%, 33%, 35.4%] missingness, respectively.\n",
    "        - Rest of the missing attributes have median missingness of 98% and a minimum of 76% missingness. \n",
    "    - To deal with this missing-ness we ran various experiments in the background.\n",
    "        - Experiment 1 \n",
    "            - We drop Drop all attributes/columns which have more that 35% missingness.\n",
    "            - This leaves us with only 13 attributes, which is 34% of the original number attributes.\n",
    "            - Imputation with mode of the training data, as all of them are categorical/ordinal.\n",
    "       - Experiment 2\n",
    "           - This experiment was insipired by 2 facts\n",
    "               1. In experiment 1 we had dropped 66% of the attributes. Which is way too many dropped attributes, 25 in number. \n",
    "                   - There is a high chance that some of these attributes have high discrimatory power, wrt to the target.\n",
    "               2. As mentioned above in the data documentation that:\n",
    "                   - '-' represents the not_applicable values\n",
    "                   - Of the 29 categorical variables, 19 have binary values.\n",
    "                   - Except \"shape\", all other 18 attributes have very high missing values.\n",
    "                   - Combining the above facts, it would makes a lot of sense to\n",
    "                       - Impute missing values with  \"not_applicable\".\n",
    "                   - However, we still drop attributes with more than 99% of missing values, which would be only 10 attributes.\n",
    "                3. We would continue to impute ['steel', 'surface_quality', 'condition', 'formability'] with training data's mode in this experiment.\n",
    "       - Experiment 3\n",
    "           - Same as experiment 2, but we also impute ['steel', 'surface_quality', 'condition', 'formability'] with \"not_applicable\".\n",
    "      \n",
    "    \n",
    "4. Since the client is interested in a high True Positivity Rate, we perform grid search based using F_beta, with more importace to recall, i.e. having the beta value set to 1.28. As per documentation of sklearn, a beta value higher than 1.0 prefers recall more than precision. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pandas/__init__.py:48\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig_init\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# dtype\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     Int8Dtype,\n\u001b[1;32m     51\u001b[0m     Int16Dtype,\n\u001b[1;32m     52\u001b[0m     Int32Dtype,\n\u001b[1;32m     53\u001b[0m     Int64Dtype,\n\u001b[1;32m     54\u001b[0m     UInt8Dtype,\n\u001b[1;32m     55\u001b[0m     UInt16Dtype,\n\u001b[1;32m     56\u001b[0m     UInt32Dtype,\n\u001b[1;32m     57\u001b[0m     UInt64Dtype,\n\u001b[1;32m     58\u001b[0m     Float32Dtype,\n\u001b[1;32m     59\u001b[0m     Float64Dtype,\n\u001b[1;32m     60\u001b[0m     CategoricalDtype,\n\u001b[1;32m     61\u001b[0m     PeriodDtype,\n\u001b[1;32m     62\u001b[0m     IntervalDtype,\n\u001b[1;32m     63\u001b[0m     DatetimeTZDtype,\n\u001b[1;32m     64\u001b[0m     StringDtype,\n\u001b[1;32m     65\u001b[0m     BooleanDtype,\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# missing\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     NA,\n\u001b[1;32m     68\u001b[0m     isna,\n\u001b[1;32m     69\u001b[0m     isnull,\n\u001b[1;32m     70\u001b[0m     notna,\n\u001b[1;32m     71\u001b[0m     notnull,\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m# indexes\u001b[39;00m\n\u001b[1;32m     73\u001b[0m     Index,\n\u001b[1;32m     74\u001b[0m     CategoricalIndex,\n\u001b[1;32m     75\u001b[0m     RangeIndex,\n\u001b[1;32m     76\u001b[0m     MultiIndex,\n\u001b[1;32m     77\u001b[0m     IntervalIndex,\n\u001b[1;32m     78\u001b[0m     TimedeltaIndex,\n\u001b[1;32m     79\u001b[0m     DatetimeIndex,\n\u001b[1;32m     80\u001b[0m     PeriodIndex,\n\u001b[1;32m     81\u001b[0m     IndexSlice,\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;66;03m# tseries\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     NaT,\n\u001b[1;32m     84\u001b[0m     Period,\n\u001b[1;32m     85\u001b[0m     period_range,\n\u001b[1;32m     86\u001b[0m     Timedelta,\n\u001b[1;32m     87\u001b[0m     timedelta_range,\n\u001b[1;32m     88\u001b[0m     Timestamp,\n\u001b[1;32m     89\u001b[0m     date_range,\n\u001b[1;32m     90\u001b[0m     bdate_range,\n\u001b[1;32m     91\u001b[0m     Interval,\n\u001b[1;32m     92\u001b[0m     interval_range,\n\u001b[1;32m     93\u001b[0m     DateOffset,\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;66;03m# conversion\u001b[39;00m\n\u001b[1;32m     95\u001b[0m     to_numeric,\n\u001b[1;32m     96\u001b[0m     to_datetime,\n\u001b[1;32m     97\u001b[0m     to_timedelta,\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;66;03m# misc\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     Flags,\n\u001b[1;32m    100\u001b[0m     Grouper,\n\u001b[1;32m    101\u001b[0m     factorize,\n\u001b[1;32m    102\u001b[0m     unique,\n\u001b[1;32m    103\u001b[0m     value_counts,\n\u001b[1;32m    104\u001b[0m     NamedAgg,\n\u001b[1;32m    105\u001b[0m     array,\n\u001b[1;32m    106\u001b[0m     Categorical,\n\u001b[1;32m    107\u001b[0m     set_eng_float_format,\n\u001b[1;32m    108\u001b[0m     Series,\n\u001b[1;32m    109\u001b[0m     DataFrame,\n\u001b[1;32m    110\u001b[0m )\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparseDtype\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtseries\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m infer_freq\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pandas/core/api.py:29\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmissing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     18\u001b[0m     isna,\n\u001b[1;32m     19\u001b[0m     isnull,\n\u001b[1;32m     20\u001b[0m     notna,\n\u001b[1;32m     21\u001b[0m     notnull,\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgorithms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     25\u001b[0m     factorize,\n\u001b[1;32m     26\u001b[0m     unique,\n\u001b[1;32m     27\u001b[0m     value_counts,\n\u001b[1;32m     28\u001b[0m )\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Categorical\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mboolean\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BooleanDtype\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfloating\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     32\u001b[0m     Float32Dtype,\n\u001b[1;32m     33\u001b[0m     Float64Dtype,\n\u001b[1;32m     34\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pandas/core/arrays/__init__.py:7\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     ExtensionArray,\n\u001b[1;32m      3\u001b[0m     ExtensionOpsMixin,\n\u001b[1;32m      4\u001b[0m     ExtensionScalarOpsMixin,\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mboolean\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BooleanArray\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcategorical\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Categorical\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatetimes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatetimeArray\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfloating\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FloatingArray\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:975\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:671\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:779\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:874\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:972\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from statistics import variance, mean\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import check_scoring\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pre-processing applied (20 marks)\n",
    "Enter the code in the cells below to execute each of the stated sub-tasks. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since the algorithms used in sklearn provide the functionality to internally convert string based categorical target to appropriate label encoding, we skip this step\n",
    "#### Instead we present here the code to read the data file and get the train, val and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_train_val_data():\n",
    "    \n",
    "    df = pd.read_csv(\"dataset/anneal.data\")\n",
    "    \n",
    "    # Because replace \"?\" represents missing. \n",
    "    df.replace(\"?\", np.nan, inplace=True)\n",
    "\n",
    "    # Because only 8 data poins have target as \"1\".\n",
    "    # Our metrics won't be reliable for this class.\n",
    "    \n",
    "    df = df[df.target != \"1\"]\n",
    "    \n",
    "    # Just a fix as some values which are supposed to be int have string representation for this column\n",
    "    df.enamelability = df.enamelability.astype(float)\n",
    "    \n",
    "    \n",
    "    df_y = df.target\n",
    "    df_X = df.drop(labels=[\"target\"], axis=1)\n",
    "    \n",
    "    # Just re-organinsing the data such that countinous columns are at the end. \n",
    "    numerical_features = [\"carbon\", \"hardness\", \"strength\", \"thick\", \"width\", \"len\"]\n",
    "    all_categorical_features = list(set(df_X.columns.to_list()) - set(numerical_features))\n",
    "    df_X = pd.concat([df_X[all_categorical], df_X[numerical_features]], axis=1)\n",
    "    \n",
    "    return df_X, df_y\n",
    "\n",
    "def get_test_data():\n",
    "    \n",
    "    df_test = pd.read_csv(\"dataset/anneal.test\")\n",
    "    df_test.replace(\"?\", np.nan, inplace=True)\n",
    "    \n",
    "    df_test.enamelability = df_test.enamelability.astype(float)\n",
    "\n",
    "\n",
    "    y_test = df_test.target\n",
    "    X_test = df_test.drop(labels=[\"target\"], axis=1)\n",
    "\n",
    "    return X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  b) Removing synonymous and noisy attributes if necessary \n",
    "\n",
    "##### All the attributes to remove will be added to a drop_features list\n",
    "##### We will maintain different lists of drop attributes for various experiments as mentioned in the Case Study Section\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = get_train_val_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product Type attribute is the same throughout the dataset, hence it can removed\n",
    "X_train.product_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display amount of missingness in attributes\n",
    "missing_means = X_train.isnull().mean().multiply(100).sort_values(ascending=False)\n",
    "missing_means.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1 drop all attributes with more the 35% missing values\n",
    "drop_attributes_exp1 = [\"product_type\"] + missing_means[missing_means > 35.0].index.to_list()\n",
    "\n",
    "# Experiment 2 drop all attributes with more the 99% missing values\n",
    "drop_attributes_exp2 = [\"product_type\"] + missing_means[missing_means > 99.0].index.to_list()\n",
    "\n",
    "# Experiment 3; same as Exp 2.\n",
    "drop_attributes_exp2 = [\"product_type\"] + missing_means[missing_means > 99.0].index.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Column Dropper Transformer for Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnDropperTransformer:\n",
    "    def __init__(self, column):\n",
    "        self.columns = column\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_new = X.drop(self.columns, axis=1)\n",
    "        return X_new\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  c) Dealing with missing values if necessary \n",
    "##### We are dealing with missing values in three different ways as mentioned in Experiment 1, 2 and 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1: Impute missingness less than 35% with mode.\n",
    "mode_imputer_list_exp1 = [\"steel\", \"shape\", \"bore\", \"surface_quality\", \"formability\"]\n",
    "\n",
    "# Experiment 2: Impute missingness 75% with a not_applicable value/category. Impute missingness less than 35% with mode.\n",
    "mode_imputer_list_exp2 = [\"steel\", \"shape\", \"bore\", \"surface_quality\", \"formability\"]\n",
    "na_imputer_list_exp2 = missing_means[(missing_means > 70.0) & (missing_means < 99.0)].index.to_list()\n",
    "\n",
    "# Experiment 3: Impute all missing value with NA category.\n",
    "na_imputer_list_exp3 = missing_means[(missing_means > 0.0) & (missing_means < 99.0)].index.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### NA imputer Transformer for Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NAColumnTransformer:\n",
    "    def __init__(self, column):\n",
    "        self.columns = column\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        without_formability = list(set(self.columns) - {'formability'})\n",
    "        \n",
    "        # Imputer string based features with string NA\n",
    "        X_new = X[without_formability].fillna(\"NA\")\n",
    "        X[wo_form] = X_new\n",
    "        \n",
    "        # Creating a new 0 category for without_formability\n",
    "        if \"formability\" in seld.columns:\n",
    "            X['formability'] = X['formability'].fillna(0).astype(int)\n",
    "        return X\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  d) Rescaling if necessary if necessary \n",
    "##### Standard Scaler Pipeline for numerical attributes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numerical_pipeline(scale=False):\n",
    "    if scale:\n",
    "        numerical_pipeline = Pipeline(steps=[('ss', StandardScaler())])\n",
    "        return numerical_pipeline\n",
    "\n",
    "    return 'passthrough'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) Categorical Feature Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case of experiment 3, simple imputer will not have any effect\n",
    "# as all the NA values would be imputed by na_imputer.\n",
    "\n",
    "def get_categorical_pipeline(cat_features_to_drop, na_imputer_cols):\n",
    "\n",
    "    categorical_pipeline = Pipeline(steps=[\n",
    "        ('drop_column', ColumnDropperTransformer(cat_features_to_drop)),\n",
    "        ('na_imputer', NAColumnTransformer(na_imputer_cols)),\n",
    "        ('mode', SimpleImputer(strategy='most_frequent')),\n",
    "        ('one-hot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "    ])\n",
    "    return categorical_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f)  Full Preprocessor Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_processeror(all_categorical_features, cat_features_to_drop, numerical_features, na_imputer_cols, scale_numerical):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param all_categorical_features: List of all the categorical features.\n",
    "    :param cat_features_to_drop: List of categorical features to drop.\n",
    "    :param numerical_features: List of numerical features.\n",
    "    :param na_imputer_cols: List of features to imputer with new \"not_applicable\" category. \n",
    "    :param scale_numerical: Boolean which controls scaling of numerical features.\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    \n",
    "    categorical_pipeline = get_categorical_pipeline(cat_features_to_drop, na_imputer_cols)\n",
    "    numerical_pipeline = get_numerical_pipeline(scale=scale_numerical)\n",
    "    full_processor = ColumnTransformer(transformers=[\n",
    "        ('category', categorical_pipeline, all_categorical_features),\n",
    "        ('numerical', numerical_pipeline, numerical_features)\n",
    "    ])\n",
    "    return full_processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 1 Data Pipeline\n",
    "- We drop Drop all attributes/columns which have more that 35% missingness.\n",
    "- We impute categorical values with mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = X_train.columns.to_list()\n",
    "numerical_features = [\"carbon\", \"hardness\", \"strength\", \"thick\", \"width\", \"len\"]\n",
    "all_categorical_features = list(set(all_features) - set(numerical_features))\n",
    "cat_features_to_drop = drop_attributes_exp1\n",
    "na_imputer_cols = []\n",
    "scale_numerical = False\n",
    "\n",
    "experiment1_data_pipeline = get_full_processeror(\n",
    "    all_categorical_features=all_categorical_features, \n",
    "    cat_features_to_drop=cat_features_to_drop, \n",
    "    numerical_features=numerical_features, \n",
    "    na_imputer_cols=na_imputer_cols, \n",
    "    scale_numerical=scale_numerical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Technique 1 (20 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Discuss your motivation for choosing the technique and provide a schematic figure of the process\n",
    "\n",
    "We use Random Forest as first technique because of the following motivations:\n",
    "\n",
    "1. The algorithm reduces the overfitting and variance problem by using bagging and ensembling.\n",
    "    - It tackles overfitting and variance by builds many decision trees with subsets of features[bagging].\n",
    "    - It then takes the majority vote of the outputs of all trees to give the final output. \n",
    "2. Reqiures no features scaling. \n",
    "3. Robustness towards outliers.\n",
    "4. The grid search is easy to perform.\n",
    "    - Unlike paramteric models, doesn't require careful tuning of regularisation parameter, as regularisation is in-built because of ensembling and bagging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter the correct code in the cells below to execute each of the stated sub-tasks.\n",
    "### b) Setting hyper parameters with rationale\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We perform a grid search on number of estimators (decision trees) and max depth of those decision trees.\n",
    "###### The number of estimators will tackle variance problem and but at the same time will increase the time complexity of the algorithm.\n",
    "###### We tune the max depth parameter casue very deep descision trees cause overfitting, but literature also suggests that, we can allow the decision trees to grow as deep as possible as long as the ensemble is large enough.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_param_grid = {\n",
    "        \"model__n_estimators\": [20, 40, 60, 80, 100, 120, 150, 200],\n",
    "        \"model__max_depth\": [5, 10, 20, 30, 40, None],\n",
    "\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Optimising hyper parameters\n",
    "We perform nested cross validation along with grid search, and return the best estimators. \n",
    "Instead of running nested CV only once, we perform many trials of it and record the best estimators on each fold, this is because the \n",
    "during nested CV the best estimator on each fold might be different, so we keep track of the best estimators during many runs and pick the \n",
    "top 3 estimators by frequency. \n",
    "\n",
    "These top 3 estimators will then be ensembled again to get the final classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_train_Fbeta_scores = []\n",
    "mean_test_Fbeta_score = []\n",
    "estimator_frequency = []\n",
    "def nestes_cross_validation(X_train, y_train, param_grid, classifier, datapipeline):\n",
    "    n_jobs = 40\n",
    "\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocess', datapipeline),\n",
    "        ('model', classifier)\n",
    "    ])\n",
    "\n",
    "    clf = GridSearchCV(estimator=rf_pipeline, param_grid=param_grid,\n",
    "                       scoring=make_scorer(recall_biased_Fbeta), cv=3, n_jobs=n_jobs, return_train_score=True)\n",
    "\n",
    "    \n",
    "    scorer = check_scoring(estimator, scoring=make_scorer(recall_biased_Fbeta))\n",
    "    cv_results = cross_validate(\n",
    "        estimator=pipeline,\n",
    "        X=X_train,\n",
    "        y=y_train,\n",
    "        scoring={\"score\": scorer},\n",
    "        cv=3,\n",
    "        n_jobs=n_jobs,\n",
    "        pre_dispatch=\"2*n_jobs\",\n",
    "        error_score=np.nan,\n",
    "        return_estimator=True,\n",
    "        return_train_score=True\n",
    "    )\n",
    "\n",
    "    run_test_Fbeta_score_mean = mean(cv_results['test_score'])\n",
    "    run_train_Fbeta_score_mean = mean(cv_results['train_score'])\n",
    "    \n",
    "    # run_var = variance(cv_results['test_score'])\n",
    "    mean_train_Fbeta_scores.append(run_train_Fbeta_score_mean)\n",
    "    mean_test_Fbeta_scores.append(run_test_Fbeta_score_mean)\n",
    "\n",
    "    for estimator in cv_results['estimator']:\n",
    "        estimator_frequency[(estimator.best_estimator_.named_steps.model.max_depth, estimator.best_estimator_.named_steps.model.n_estimators)] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_jobs=n_jobs, oob_score=False)\n",
    "for i in range(5):\n",
    "        grid_search(X_train.copy(), y_train.copy(), rf_param_grid, rf, experiment1_data_pipeline)\n",
    "        print(\"OVERALL: \", mean(mean_train_Fbeta_scores), mean(mean_test_Fbeta_score))\n",
    "        print(estimator_count)\n",
    "        print(\"*\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Performance metrics for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_biased_Fbeta(y_true, y_pred):\n",
    "    fbs = fbeta_score(y_true, y_pred, average='macro', beta=1.28)\n",
    "    return fbs\n",
    "\n",
    "def print_recall_biased_Fbeta(y_true, y_pred):\n",
    "    print(classification_report(y_true, y_pred, zero_division=0))  # print classification report\n",
    "    fbs = fbeta_score(y_true, y_pred, average='macro', beta=1.28)\n",
    "    print(\"F1_beta: \", fbs)\n",
    "    print(\"\\n***---***\\n\")\n",
    "    return fbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Technique 2 (20 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Discuss your motivation for choosing the technique and  provide a schematic figure of the process\n",
    "\n",
    "100-200 words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter the correct code in the cells below to execute each of the stated sub-tasks.\n",
    "### b) Setting hyper parameters with rationale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Optimising hyper parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Performance metrics for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparison of metrics performance for testing (16 marks)\n",
    "Enter the correct code in the cells below to execute each of the stated sub-tasks. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Use of cross validation for both techniques to deal with over-fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Comparison with appropriate metrics for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Model selection (ROC or other charts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final recommendation of best model (8 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Discuss the results from a technical perspective, for example, overfitting discussion, complexity and efficiency\n",
    "\n",
    "100-200 words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Discuss the results from a business perspective, for example, results interpretation, relevance and balance with technical perspective\n",
    "\n",
    "100-200 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion (8 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) What has been successfully accomplished and what has not been successful?\n",
    "100-300 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Reflecting back on the analysis, what could you have done differently if you were to do the project again?\n",
    "\n",
    "100-300 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Provide a wish list of future work that you would like to do\n",
    "\n",
    "100-200 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
